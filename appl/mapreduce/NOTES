the idea is we define the interfaces for a mapreduce framework. Then
we can try several different implementations of it, but the actual functions
passed to map and reduce remain the same.

we are abstracting out the way we iterate over a large number of files,
so that this work can be parrallelized and optimized.



given a list of files, maybe one directory, or a  root of a filetree or a whole filesystem. we want to perform an action on each file.


we are hiding the intermediate files, and trying to keep the total memory
used to aggregate low.

mapred {mapfn} {reducefn}  /fs

mapred {tokfreq} {agg}  /fs

The reduce function must be a filter, that takes 'key val' pairs one per line.
The map function is run once per file and takes the filename as argument.

Essentially we have a large list of files, on which we run the map function on each file. It writes to standard output a 'key val' pair. mapred will take care
of storing into files and running the reduce function. 

The reduce function expects the 'key val' pairs to be ordered, by key. 

The simplest implementation is to walk the tree and run mapfn on each file,
sort all the output and redirect to reduce.  and we get one file as output.

e.g.

fn mapred {
	map = $1
	reduce = $2
	dir = $3
	t = /tmp/mapred.${pid}
	mkdir $t
	rm $t/* >[2] /dev/null
	fs pipe @$map $dir |intermediate 13 $t/
	for i in $t/* {sort $i | $reduce > $i.red}
}


The above is the simplest solution. 

We could start it in it's own emu. We could split it among a number of emus.
We could improve the merging. And do some analysis of the filesystem to
decide how to split it.

The result could be a filesystem that can be passed to another mapred function.
We should specify the output filesystem as part of the interface.


The implementation changes depending on the environment and infrastructure.
one implementation would work well on a single emu instance. another could be created for a huge cluster. but the interface to the user and the commands used remains the same.

We could try splitting the data and have /tmp/mapred.pid/Msplit/Rintermediate


We could define the map and reduce functions with specific interfaces using
channels to receive input.

the map and reduce functions should not be aware of the format stored on disk.

map(key, value)
	emits a (key, value), both strings, onto a channel

for map, key is typically a filename, value is implicit and is the contents of the file.
else it's already the open Iobuf. We should deal with errors consistently.

reduce(key, list)
	recieves list on channel which it iterates until done 
	emits a value (or nothing) then returns


reader(emit: chan of (key, value));
	the reader reads the list of files from stdin and decides
	how to parse them. different readers can read different file types.
	the reader emits the key value pairs which the master
	hands off to map workers which it spawns
	the workers writer to intermediate workers that are also
	watched by the master.

master
	contains worker list, and intermediate list.
	once workers are done for an intermediate, the intermediate
	can be killed.
	once all workers and intermediates are done we exit.

we need to watch a lot of processes.


we need a command that given a file system /n/mbox will 
look at the size of all the files and split it into equal lists of files.
e.g. divide /n/mbox by 16 we'd get 16 lists of files
each with about equal total size.

we'd need to do that by file sizes assuming a certain order,
and starting files by offset, or just round up.
get the total size, divide by 16 and break into equal chunks
>= than 16.

start with one reader, one map, one intermediate.


we need a re-entrant buffered IO.


The thing to expand first is have the reduce worker gather multiple
files from multiple machines for the same hash partition, and
do an external sort on that file.
we still need to be able to sort large files with a bound on memory.


We need a gather and sort function before reducer is called.

The external sort is probably the most pressing thing to work on.
We shouldn't improve anything else until we have that.

gather streams all the lines into sorter which writes to disk and
merges as necessary. once gather finishes, sort merges
all the temporary files and writes out to one reduce partition.


we can then create the remote job management system.
we need a Worker interface that takes a job specification
and runs and logs what it does and reports back to the master its
progress.

The worker might take the name of the module to load, the files and
file splits on which to run. It reports back the files it creates once
it's done with them and responds to heartbeats from the master,
and reports nbytes processed.

Master launches Worker.
Master write job configuration to file.
Worker reads file and write progress to /n/client/master
which is a channel served by the master.

Master should be a single fs that all the clients assigned to one
ctl file.
Master 	       1/
			/ctl
			/data

This filesystem is served for each remote host where we plan on launching any jobs
using the rstyx protocol. 

The cfg just contains the modules to load and the filenames and splits.

The Master launches each worker 
	Worker /n/client/1

Worker opens ctl and gets a new connection to the Master. It reads the cfg file
and loads the named modules. E.g. the reader and mapper. then it processes 
the files and passes them to reader. Reader passed them to mapper. The
output files are written back to the data channel to master.

All the clients are kept track of as open fids, just like a normal fs. If we loose
a client we know. We can restart clients if we loose them and fail to get
a proper close out message etc.

A Masterfs is the way to go.

The Masterfs could as the registry for where to go to push the load.

The rstyx on each server can update it's registry entry for each connection
so the load is properly controlled. E.g. with max number of jobs.
Or max number of connections.

Start with modfs.b
	clone
	1
		ctl
		data


Master wants to run job, say a mapper worker
	it looks up registry for available rstyx resource 
		(anything without a maxed out number of connections)
	using rstyx protocol it connects to the server, writes the Worker command	
	then it begins serving it's own fs on the connection
		(or it can bind its connection onto its own namespace and serve that).
	Worker knows to just open /n/client/mnt/master/clone and read the number of its connection.
	It then opens the ctl and data files. It uses data as it's stdin/stdout.
	From ctl it get heartbeat or writes stderr.
	Worker reads the configuration and starts the job,
	when it's done it closes its connection
	The master can mark off that job as done


The registry must change slightly to become a job exchange. A master asks the
registry for a resource to run one connection, one job. The registry looks up a candidate
and gives it out. But now there's a lag between the master connecting and the resource writing back to the registry that a connection's been opened. One way around it is just for the resource to just turn it away, so the master needs to keep coming back. And if there are no resources, what does the master do? just periodically poll the registry?


grid/registry already practically does what we want with monitoring the number of connections. It just needs to write it back to the registry.



Tasks
1. Implement External Sorter
2. Implement Resource partition gatherer
3. Modify grid/register* commands to update registry with number of connections and other resource usage.
4. Write generic worker and come up with filesystem interface
5. Write code to split large set of files into equal size chunks
6. Write Masterfs to spawn and monitor workers


Gather is really just cat. we can script a lot of this.
mapred needs to call a script to gather and sort the files.
Mapworkers and Reduceworkers are also standalone commands.

The result of a mapworker is a list of files.
The input to a mapworker is a list of filesplits.
	the input also includes a config file that specifies the mapper and reader
	modules.

The input to a reduceworker is the list of intermediate files.
	it calls the gather and sort command to create a local file
	for it to reduce. the gathersort manages mounting the fs
	as needed.

The output of reduceworker is the reduce file.

We need a filesplitter that can create a set of mapworker config files.
It walks a fs calculates total size and splits on a set quanta.
Much like du.

Create netcat that takes as stdin
	tcp!host!port	/path/file

it builds a hash of network address and mounts one at a time
and cats the /path/file

assuming the full list of filenames can be stored in memory.
or we can just do a fresh mount for each file.

no bottlenecks, keep the data moving. the reduce worker
can be running concurrently with the mapworkers, because
the sorting can happen while we are receiving intermediate
file names from finished mapworkers.

netcat < files |sort |reduceworker cfgfile > out


we will end up with a Family of mapreduce programs
for different scales of data and grids.

Implement a Reader for tar.gz files.
